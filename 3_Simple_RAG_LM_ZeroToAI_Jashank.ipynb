{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e78c5067",
   "metadata": {},
   "source": [
    "# **Simple RAG Application** | Zero-To-AI\n",
    "#### By Jashank Kshirsagar\n",
    "#### **Connect with me on LinkedIn**: [linkedin.com/in/jashank-kshirsagar](https://www.linkedin.com/in/jashank-kshirsagar/)\n",
    "This notebook shows how to build the simplest Retrieval-Augmented Generation (RAG) system.\n",
    "It uses TF-IDF (a very basic text retrieval method) and Microsoft's Phi-2 model (a small language model).\n",
    "Our Goal: Ask a question → find the most relevant text chunks → feed them to Phi-2 → get a grounded answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6778b842",
   "metadata": {},
   "source": [
    "### **Reccomended Prerequisites:**  \n",
    "- To really understand what the following script does, you must first understand what RAG is. \n",
    "- If you aren't familiar with Transformer models, I reccomend starting with Notebooks 1 and 2 in this series. \n",
    "- Come back to this script once you have read the following article:  \n",
    "1. https://aws.amazon.com/what-is/retrieval-augmented-generation/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b20b8e3",
   "metadata": {},
   "source": [
    "### **Required Library Installs in Terminal :**    \n",
    "pip install transformers  \n",
    "pip install torch  \n",
    "pip install scikit-learn  \n",
    "pip install re  \n",
    "pip install numpy  \n",
    "pip install typing  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8bfe29",
   "metadata": {},
   "source": [
    "**STEP 1: CREATE THE KNOWLEDGE BASE**  \n",
    "These are the \"documents\" we want to use as the knowledge source. In a real project, these could be PDFs, policies, textbooks, etc.   \n",
    "For simplicity, we will hardcode a small list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee39c8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Tuple             # Used to describe the input/output types of functions (helps readability)\n",
    "import re                                  # Python's \"regular expressions\" library (for splitting text into sentences)\n",
    "import numpy as np                         # Numerical library for working with arrays (used in similarity sorting)\n",
    "\n",
    "# Libraries for retrieval (finding relevant text)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Converts text into TF-IDF vectors (bag-of-words style)\n",
    "from sklearn.metrics.pairwise import cosine_similarity        # Measures similarity between vectors (query vs chunks)\n",
    "\n",
    "# Libraries for generation (using Phi-2 to generate answers)\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline  # Hugging Face tools to load/generate text\n",
    "\n",
    "# Knowledge Base (our documents)\n",
    "DOCS: List[str] = [\n",
    "    \"RAG means Retrieval-Augmented Generation. Instead of relying on the model's internal memory, we first retrieve relevant text from our documents, then pass that text to the model to ground the answer.\",\n",
    "    \"A minimal RAG has three steps: chunk the documents into small pieces, retrieve the top-k relevant chunks for a question, and generate an answer using the chunks as context.\",\n",
    "    \"Chunk size matters. If chunks are too small, you lose context. If they are too big, you add noise. Start with a few sentences per chunk and adjust based on results.\",\n",
    "    \"TF-IDF is a simple way to vectorize text for retrieval. It is fast, understandable, and good enough for small demos.\",\n",
    "    \"Always show citations for trust. Print which chunks you used so the reader can see the source of the answer.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9276f8f",
   "metadata": {},
   "source": [
    "**STEP 2: SPLITTING DOCUMENTS INTO CHUNKS**  \n",
    "Large documents are too big to feed into a model directly. So we break them into smaller pieces (\"chunks\") that the retriever can search through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c155115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_split(text: str) -> List[str]:  # Splits a document into individual sentences\n",
    "    sents = re.split(r'(?<=[.!?])\\s+', text.strip())  # Regex: split whenever there's a period/question mark/exclamation\n",
    "    return [s for s in sents if s]  # Return sentences, removing any empty results\n",
    "\n",
    "def chunk_doc(text: str, chunk_size: int = 3, overlap: int = 1) -> List[str]:\n",
    "    sents = sentence_split(text)  # First, split the text into sentences\n",
    "    chunks = []  # Create an empty list to hold the chunks\n",
    "    i = 0  # Start from the first sentence\n",
    "    while i < len(sents):  # Keep going until we reach the end of sentences\n",
    "        chunk = \" \".join(sents[i:i + chunk_size])  # Take \"chunk_size\" sentences at a time\n",
    "        if chunk.strip():  # If the chunk isn't empty, keep it\n",
    "            chunks.append(chunk)\n",
    "        i += max(1, chunk_size - overlap)  # Move the window forward, leaving some overlap\n",
    "    return chunks  # Return all chunks created from this text\n",
    "\n",
    "def build_corpus(docs: List[str], chunk_size: int = 3, overlap: int = 1) -> Tuple[List[str], List[Tuple[int, int]]]:\n",
    "    corpus, meta = [], []  # corpus = list of all chunks, meta = where each chunk came from\n",
    "    for d_i, text in enumerate(docs):  # Go through each document\n",
    "        chs = chunk_doc(text, chunk_size=chunk_size, overlap=overlap)  # Break into chunks\n",
    "        for c_i, c in enumerate(chs):  # For each chunk...\n",
    "            corpus.append(c)  # Add the chunk text to the corpus\n",
    "            meta.append((d_i, c_i))  # Record (document index, chunk index)\n",
    "    return corpus, meta  # Return both the chunks and the metadata\n",
    "\n",
    "CORPUS, META = build_corpus(DOCS, chunk_size=3, overlap=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6191fdc",
   "metadata": {},
   "source": [
    "**STEP 3: CONVERTING CHUNKS TO VECTORS**  \n",
    "We use TF-IDF for this step.  \n",
    "- TF-IDF = Term Frequency - Inverse Document Frequency\n",
    "\n",
    "It gives higher weight to words that are frequent in one document but rare across all documents.  \n",
    "Example: \"RAG\" will get a higher score than \"the\" or \"and\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29507425",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "VECT = TfidfVectorizer(ngram_range=(1, 2))  # (1,2) means we consider single words and 2-word phrases\n",
    "CHUNK_VECS = VECT.fit_transform(CORPUS)     # Learn vocabulary + turn each chunk into a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6916d25d",
   "metadata": {},
   "source": [
    "**STEP 4: RETRIEVE RELEVANT CHUNKS**  \n",
    "Given a question, we turn it into a vector, then find the most similar chunk vectors using cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37d1ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query: str, k: int = 3) -> List[int]:\n",
    "    q_vec = VECT.transform([query])                 # Convert the query into a TF-IDF vector\n",
    "    sims = cosine_similarity(q_vec, CHUNK_VECS)[0]  # Compare query with every chunk → get similarity scores\n",
    "    order = np.argsort(-sims)                       # Sort chunk indices from highest to lowest similarity\n",
    "    return order[:k].tolist()                       # Return top-k most similar chunk IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f035cd50",
   "metadata": {},
   "source": [
    "**STEP 5: LOADING THE LLM**  \n",
    "Phi-2 is a small but powerful open-source language model from Microsoft. It works on CPU for small tasks, making it a great choice even if you don't have a GPU. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313f70a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_MODEL = \"microsoft/phi-2\"                                   # Model name from Hugging Face\n",
    "tok = AutoTokenizer.from_pretrained(GEN_MODEL)                  # Load tokenizer (turns text → tokens → text)\n",
    "lm = AutoModelForCausalLM.from_pretrained(GEN_MODEL)            # Load model weights into memory\n",
    "gen = pipeline(\"text-generation\", model=lm, tokenizer=tok)      # Build a simple text generator pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a9c087",
   "metadata": {},
   "source": [
    "**STEP 6: BUILDING THE SYSTEM PROMPT**  \n",
    "The difference with RAG: instead of just asking the question, we insert the retrieved chunks into the prompt.  \n",
    "That way, the model \"reads\" the context and uses it to give a more grounded and factually accurate answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d036826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question: str, top_chunk_ids: List[int], max_chars: int = 1200) -> str:\n",
    "    ctx_blocks, running = [], 0  # ctx_blocks = the retrieved chunks we’ll insert, running = track prompt size\n",
    "    for cid in top_chunk_ids:  # For each retrieved chunk ID...\n",
    "        block = f\"[{cid}] {CORPUS[cid]}\"  # Label the chunk with its ID for citation\n",
    "        if running + len(block) > max_chars:  # If adding this makes the prompt too long, stop\n",
    "            break\n",
    "        ctx_blocks.append(block)  # Otherwise, keep the chunk\n",
    "        running += len(block)     # Update the running total of characters\n",
    "    context = \"\\n\".join(ctx_blocks)  # Join all chunks into one block of text\n",
    "    prompt = (\n",
    "        \"You are a helpful assistant. Answer the question using ONLY the context.\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    return prompt  # Return the final input string for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f28f727",
   "metadata": {},
   "source": [
    "**STEP 7: GENERATE AN ANSWER**  \n",
    "Finally! Phew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe2860a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(question: str, k: int = 3, max_new_tokens: int = 120) -> Tuple[str, List[int]]:\n",
    "    top_ids = retrieve(question, k=k)  # Step 1: retrieve top-k chunks\n",
    "    prompt = build_prompt(question, top_ids)  # Step 2: build prompt with those chunks\n",
    "    out = gen(prompt, max_new_tokens=max_new_tokens, do_sample=False)[0][\"generated_text\"]  # Step 3: generate text\n",
    "    ans = out.split(\"Answer:\", 1)[-1].strip()  # Clean output: only keep what's after \"Answer:\"\n",
    "    return ans, top_ids  # Return both the final answer and the IDs of chunks used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426796c6",
   "metadata": {},
   "source": [
    "**NOW GIVE IT A TRY**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675303de",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is RAG and why do we chunk documents?\"  # Replace this with a question of your choice\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    q = question  # Example question\n",
    "    a, used = answer(q, k=3, max_new_tokens=120)      # Run the RAG pipeline\n",
    "    print(\"Q:\", q)                                   # Print the question\n",
    "    print(\"\\nA:\", a)                                 # Print the model's answer\n",
    "    print(\"\\nCitations:\")                             # Print citations for transparency\n",
    "    for cid in used:                                 # For each used chunk...\n",
    "        doc_id, chunk_id = META[cid]                 # Look up which document + chunk it came from\n",
    "        snippet = CORPUS[cid][:120].replace(\"\\n\", \" \")  # Show only the first 120 chars as a preview\n",
    "        print(f\" - [chunk {cid} | doc {doc_id} | part {chunk_id}] :: {snippet}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c4b4e3",
   "metadata": {},
   "source": [
    "✅ You have now built your very first **Retrieval-Augmented Generation (RAG) system** from scratch.  \n",
    "- You learned how to split documents into chunks.  \n",
    "- You used TF-IDF to retrieve relevant chunks for a query.  \n",
    "- You built a simple prompt and generated an answer with **Phi-2**.  \n",
    "- You also saw how to display **citations** so users can trust the source of the answer.  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
