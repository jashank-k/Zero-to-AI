{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bf98eaf",
   "metadata": {},
   "source": [
    "# **2. Building a Simple Language Model** | Zero-To-AI\n",
    "#### by Jashank Kshirsagar\n",
    "#### **Connect with me on LinkedIn:**  linkedin.com/in/jashank-kshirsagar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50c3bcc",
   "metadata": {},
   "source": [
    "### **Reccomended Prerequisites:**  \n",
    "To really understand what the following script does, you must first understand what an Encoder and Decoder are, as well as the architecture of a Transformer model. Come back to this script once you have read the following articles:  \n",
    "1. https://medium.com/@amanatulla1606/transformer-architecture-explained-2c49e2257b4c  \n",
    "2. https://kikaben.com/transformers-encoder-decoder/  \n",
    "\n",
    "If you've taken the time to read through those, the next part will make a LOT more sense to you!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83010a93",
   "metadata": {},
   "source": [
    "### **Required Library Installs in Terminal :**  \n",
    " pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121   \n",
    " pip install transformers  \n",
    " pip install ipykernel  \n",
    " pip install langchain  \n",
    " pip install huggingface_hub  \n",
    " pip install langchain_community  \n",
    "\n",
    " **Note:** If your cells do not run at any point, 'pip uninstall' the library first and 'pip install it again'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ef0623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTING MODULES AND CLASSES\n",
    "import torch #Needed to handle model weights with PyTorch   \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline \n",
    "# transformers is a huggingface library - free open source transformers\n",
    "# 'AutoModelForCausalLM' - loads a pretrained language model \n",
    "#'AutoTokenizer' - Loads the correct tokenizer for the chosen model\n",
    "#'pipeline' - one line of code can create a pipeline for the task\n",
    "from langchain.prompts import PromptTemplate #Provides a prompt template for LM tasks. Makes code cleaner when using chatbots\n",
    "from langchain.chains import LLMChain #Helps connect a prompt template and language model. Fills the prompt, sends it to model and collects the output\n",
    "from langchain_community.llms import HuggingFacePipeline #Allows to use hugging face models (through a pipeline from transformers). Plugs locally run Hugging Face models into the LangChain ecosystem. (GPT is on clod, Huggung face runs local, hence no payment req)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc86aedf",
   "metadata": {},
   "source": [
    "**STEP 1: CHOOSING AND LOADING THE LANGUAGE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86da240",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"microsoft/phi-2\" # We're using 'microsoft/phi-2' because it's small and runs well on most computers.\n",
    "\n",
    "#Loading the tokenizer to help the model understand words\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "#Loading the actual language Model\n",
    "my_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, #The variable we specified at the top, which you can simply change to test other models\n",
    "    torch_dtype = torch.float16, #This data type helps save memory (smaller bit size)\n",
    "    trust_remote_code = True #Allows loading custom code from the model's repository\n",
    ")\n",
    "\n",
    "print (\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5cd179",
   "metadata": {},
   "source": [
    "**STEP 2: SETTING UP THE TEXT GENERATION PIPELINE**  \n",
    "In this step we create a tool that takes user input and generates text using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028168e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = pipeline(\n",
    "    \"text-generation\", #This pipeline helps in text generation\n",
    "    model = my_model,\n",
    "    tokenizer = tokenizer,\n",
    "    max_new_tokens = 100 , #limits the length of the generated answer to 100 tokens\n",
    "    temperature = 0.5,  #How creative/random the answer is (0.0= very precise, 1.0= very creative)\n",
    "    do_sample = False, #Allows the model to pick words randomly based on probability\n",
    "    device_map = \"auto\" #Automatically use GPU if available, otherwise CPU ('cuda:0'=GPU otherwise CPU)\n",
    ")\n",
    "print (\"Text generation pipeline created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928b8770",
   "metadata": {},
   "source": [
    "**STEP 3: INTEGRATION WITH LANGCHAIN**  \n",
    "LangChain is a Python Library that makes it easy to build powerful applications with language models. It has specific tools that help in this task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5fb009",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_template = \"{question}\"     #This is our template string. {question} is a placeholder which will be replaced by the actual text string variable 'question'\n",
    "\n",
    "prompt = PromptTemplate(template= prompt_template, input_variables=[\"question\"] ) #Creating a Langchain Prompt template from our template string \n",
    "\n",
    "llm= HuggingFacePipeline(pipeline = text_pipeline) #This connects our HuggingFace pipeline to LangChain\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=False) #creating the actual prompt to llm to response pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a552dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 4: ASKING QUESTION AND GETTING THE ANSWER\n",
    "\n",
    "question= \"What is a Language Model?\" #The Actual Question\n",
    "response = llm_chain.run(question) #running the question entered above through the LM pipeline created\n",
    "\n",
    "print(\"Response:\", response) #The response generated by the model\n",
    "\n",
    "\n",
    "#NOTE: If you're running this on CPU, responses will take a few minutes. Be patient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454d9b03",
   "metadata": {},
   "source": [
    "- You've just created a functional Python script that downloads and runs an open-source large language model locally, enabling you to ask questions and receive generated text responses, all managed efficiently through the LangChain library.  \n",
    "\n",
    "- You now understand the fundamental steps and components involved in setting up and interacting with a local, open-source Large Language Model.\n",
    "\n",
    "- Try testing your own prompt or even better, a different model to see how your outputs differ. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f5d366",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jk-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
